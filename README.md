# IA DCI

Suite d'assistants linguistiques internes DCI pour traduire, corriger, reformuler et r√©sumer vos contenus gr√¢ce aux mod√®les Ollama.

## üéØ Fonctionnalit√©s

- ‚úÖ **Traduction instantan√©e** : Texte source √† texte cible (FR/EN/AR) avec synchronisation des zones de saisie
- ‚úÖ **Correction orthographique** : Texte corrig√© pr√™t √† copier, avec explications des modifications
- ‚úÖ **Reformulation** : Reformulation professionnelle avec points cl√©s mis en √©vidence
- ‚úÖ **Compte rendu** : G√©n√©ration de r√©sum√©s de r√©union structur√©s (r√©sum√©, d√©cisions, actions)
- ‚úÖ **Support multilingue** : Fran√ßais, Anglais, Arabe (avec gestion RTL)
- ‚úÖ **Interface web moderne** : Simple, responsive et intuitive
- ‚úÖ **Robustesse** : Retries exponentiels, circuit-breaker, timeouts
- ‚úÖ **Observabilit√©** : Logs structur√©s JSON, m√©triques, healthcheck
- ‚úÖ **D√©ploiement facile** : Docker Compose pr√™t √† l'emploi
- ‚úÖ **Sans stockage** : Traitement en m√©moire, aucune persistance

## üìã Pr√©requis

- Docker et Docker Compose
- Un serveur IA Ollama accessible (sur un serveur distant ou local)
- Un mod√®le Ollama install√© (ex: `mistral-small3.2:latest`)

> [!NOTE]
> L'application et le serveur IA Ollama peuvent √™tre d√©ploy√©s sur des serveurs **distincts**. Cette s√©paration permet une meilleure allocation des ressources (GPU pour l'IA, CPU classique pour l'application).

## üöÄ Installation et d√©ploiement

### D√©ploiement rapide avec Docker Compose

1. **Cloner ou t√©l√©charger le projet** :

```bash
cd tradia
```

1. **Configurer les variables d'environnement** (optionnel) :

```bash
cp .env.example .env
# √âditer .env selon vos besoins
```

1. **Construire et d√©marrer l'application** :

```bash
docker compose up -d --build
```

1. **V√©rifier le d√©ploiement** :

```bash
# V√©rifier les logs
docker compose logs -f

# Tester le healthcheck (via Traefik)
curl -k https://localhost/healthz
```

1. **Acc√©der √† l'application** :
Ouvrir votre navigateur √† : **<https://localhost>**

Traefik g√®re la terminaison TLS sur le port 443 et redirige automatiquement le trafic HTTP (port 80) vers HTTPS.

Les certificats TLS attendus par Traefik sont mont√©s depuis l'h√¥te :

- Dossier h√¥te : `/etc/ssl/tradia`
- Fichiers requis : `tradia.cer` et `tradia.key`

Ces fichiers sont expos√©s dans le conteneur Traefik sous `/etc/traefik/certs`, conform√©ment au `docker compose.yml`.

### Architecture recommand√©e : Application et IA sur serveurs distincts

Pour une utilisation en production, il est recommand√© de d√©ployer l'application et le serveur IA Ollama sur des serveurs s√©par√©s :

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         HTTPS/API Key          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Serveur Applicatif‚îÇ ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ    Serveur IA       ‚îÇ
‚îÇ   (Tradia + Traefik)‚îÇ                                ‚îÇ (Ollama + Traefik)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### 1. D√©ployer le serveur IA (sur le serveur avec GPU)

```bash
# Sur le serveur IA
docker compose -f docker-compose.ai.yml up -d
```

Ce fichier d√©ploie :

- **Ollama** : serveur de mod√®les IA
- **Traefik** : reverse proxy avec terminaison HTTPS et authentification par API Key

#### 2. Configurer le serveur applicatif

```bash
# Dans votre fichier .env sur le serveur applicatif
OLLAMA_BASE_URL=https://IP_SERVEUR_IA
OLLAMA_API_KEY=votre_cle_api_secrete
```

Ou directement dans `docker-compose.yml` :

```yaml
environment:
  - OLLAMA_BASE_URL=https://IP_SERVEUR_IA
  - OLLAMA_API_KEY=votre_cle_api_secrete
```

> [!IMPORTANT]
> Le serveur IA est prot√©g√© par une API Key. Assurez-vous que la m√™me cl√© est configur√©e c√¥t√© serveur IA (`docker-compose.ai.yml`) et c√¥t√© application.

### D√©ploiement derri√®re un proxy

Si votre infrastructure n√©cessite un proxy :

```bash
# Dans votre fichier .env
HTTP_PROXY=http://proxy.example.com:8080
HTTPS_PROXY=http://proxy.example.com:8080
NO_PROXY=localhost,127.0.0.1
```

### Installation et v√©rification d'Ollama (sur le serveur IA)

```bash
# V√©rifier qu'Ollama est bien accessible
curl http://localhost:11434/api/tags

# Si Ollama n'est pas install√©, installez-le :
curl -fsSL https://ollama.com/install.sh | sh

# T√©l√©charger un mod√®le
ollama pull mistral-small3.2:latest

# V√©rifier les mod√®les disponibles
ollama list
```

## üîß Configuration

### Variables d'environnement

| Variable | D√©faut | Description |
|----------|--------|-------------|
| `OLLAMA_BASE_URL` | `http://localhost:11434` | URL du service Ollama (distant ou local) |
| `OLLAMA_API_KEY` | - | Cl√© API pour authentification au serveur IA |
| `OLLAMA_MODEL` | `mistral-small3.2:latest` | Mod√®le par d√©faut |
| `OLLAMA_TIMEOUT` | `120` | Timeout en secondes |
| `OLLAMA_MAX_RETRIES` | `3` | Nombre de retries |
| `MAX_UPLOAD_MB` | `50` | Taille max fichier (MB) |
| `BATCH_SIZE` | `10` | Taille des lots de traduction |
| `LOG_LEVEL` | `INFO` | Niveau de log |
| `DEBUG` | `false` | Mode debug |

### Personnalisation du mod√®le

Vous pouvez utiliser diff√©rents mod√®les Ollama :

```bash
# Installer d'autres mod√®les
ollama pull mistral
ollama pull codellama

# Utiliser dans l'interface ou modifier la config
OLLAMA_MODEL=mistral
```

## üìñ Utilisation

### Via l'interface web

1. Choisir le **mod√®le Ollama** (Mistral Small par d√©faut).
2. S√©lectionner l'onglet correspondant √† votre besoin :
   - üìù Traduction : choisissez les langues source/cible puis collez votre texte.
   - ‚úÖ Correction : collez votre texte pour obtenir la version corrig√©e et les explications.
   - ‚ôªÔ∏è Reformulation : collez votre texte pour une reformulation professionnelle.
   - üóÇÔ∏è Compte rendu : collez vos notes de r√©union pour g√©n√©rer un r√©sum√© structur√©.
3. Cliquez sur le bouton de l'onglet pour lancer l'analyse.
4. Copiez le r√©sultat ou t√©l√©chargez les √©l√©ments utiles (r√©sum√©, d√©cisions, actions).

Les zones de texte de la traduction sont synchronis√©es pour faciliter la comparaison entre l'original et le r√©sultat.

## üîç Observabilit√©

### Healthcheck

```bash
curl -k https://localhost/healthz
```

R√©ponse :

```json
{
  "status": "healthy",
  "ollama_available": true,
  "ollama_url": "http://localhost:11434"
}
```

### M√©triques

```bash
curl -k https://localhost/metrics
```

R√©ponse :

```json
{
  "text_translations": 10,
  "corrections": 5,
  "reformulations": 3,
  "meeting_summaries": 2
}
```

### Logs

Les logs sont structur√©s en JSON pour faciliter l'analyse :

```bash
# Voir les logs en temps r√©el
docker compose logs -f tradia

# Filtrer par niveau
docker compose logs tradia | grep ERROR
```

## üß™ Tests

### Ex√©cuter les tests

```bash
# Dans le conteneur
docker compose exec tradia pytest

# Avec coverage
docker compose exec tradia pytest --cov=app --cov-report=html

# Tests sp√©cifiques
docker compose exec tradia pytest tests/ -v
```

### Tests locaux (sans Docker)

```bash
# Cr√©er un environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# Installer les d√©pendances
pip install -r requirements.txt

# Lancer les tests
pytest
```

## üõ†Ô∏è Maintenance

### Mise √† jour de l'application

```bash
# Arr√™ter l'application
docker compose down

# Mettre √† jour le code
git pull  # si vous utilisez git

# Reconstruire et red√©marrer
docker compose up -d --build
```

### V√©rifier l'√©tat

```bash
# √âtat des conteneurs
docker compose ps

# Utilisation des ressources
docker stats tradia

# Espace disque
docker system df
```

### Nettoyage

```bash
# Arr√™ter et supprimer les conteneurs
docker compose down

# Supprimer les volumes (si cr√©√©s)
docker compose down -v

# Nettoyer les images inutilis√©es
docker image prune -a
```

## üêõ D√©pannage

### Ollama n'est pas accessible

**Si Ollama est sur le m√™me serveur :**

```bash
# V√©rifier qu'Ollama est d√©marr√©
systemctl status ollama

# Red√©marrer Ollama
systemctl restart ollama

# Tester la connexion
curl http://localhost:11434/api/tags
```

**Si Ollama est sur un serveur distant :**

```bash
# V√©rifier la connectivit√© HTTPS
curl -k https://IP_SERVEUR_IA/api/tags \
  -H "Authorization: Bearer VOTRE_API_KEY"

# V√©rifier les logs du conteneur IA
docker compose -f docker-compose.ai.yml logs -f
```

### L'application ne d√©marre pas

```bash
# V√©rifier les logs
docker compose logs tradia

# V√©rifier la configuration
docker compose config

# Reconstruire depuis z√©ro
docker compose down
docker compose build --no-cache
docker compose up -d
```

### Erreur de traduction

1. V√©rifier les logs pour identifier le segment probl√©matique
2. V√©rifier que le mod√®le Ollama est bien charg√© : `ollama list`
3. Tester manuellement la traduction avec Ollama
4. V√©rifier la taille du fichier (< 50MB par d√©faut)

### Performance lente

1. Augmenter les ressources Docker :

```yaml
deploy:
  resources:
    limits:
      cpus: '4'
      memory: 4G
```

1. R√©duire la taille des lots :

```bash
BATCH_SIZE=5
```

1. Utiliser un mod√®le plus petit/rapide

## üìä Architecture technique

### Stack technologique

- **Backend** : FastAPI + Uvicorn
- **HTTP** : httpx (avec retries)
- **Templating** : Jinja2
- **Tests** : pytest
- **Conteneurisation** : Docker

### Robustesse du client Ollama

- ‚úÖ Retries exponentiels (backoff 2^n)
- ‚úÖ Circuit breaker (arr√™t apr√®s 5 √©checs)
- ‚úÖ Timeout configurable
- ‚úÖ Support proxy
- ‚úÖ Logs structur√©s JSON

## üìù Licence

Ce projet est d√©velopp√© pour un usage interne DSI.

## ü§ù Support

Pour toute question ou probl√®me :

1. Consulter cette documentation
2. V√©rifier les logs : `docker compose logs`
3. Tester le healthcheck : `curl -k https://localhost/healthz`
4. Contacter l'√©quipe infrastructure DSI

## üîÑ Roadmap

- [ ] Interface d'administration
- [ ] Authentification LDAP/SSO
- [ ] API REST document√©e (Swagger)
- [ ] Traductions en cache (Redis)

---

**Version** : 2.0.0  
**Derni√®re mise √† jour** : 2025-12-08  
**Responsable** : Infrastructure DSI
