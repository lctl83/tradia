# Fichier d'exemple de configuration
# Copier en .env et remplir les valeurs

# ===== PRODUCTION =====
# URL du serveur IA (HTTPS)
OLLAMA_BASE_URL=https://itapprspia01.dci.local/api

# Clé API (demander à l'administrateur)
OLLAMA_API_KEY=

# Modèle par défaut
OLLAMA_MODEL=ministral-3:latest

# ===== TESTS LOCAUX =====
# Décommenter pour utiliser Ollama local
# OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_MODEL=ministral-3:3b-cloud

# ===== PROXY (optionnel) =====
# HTTP_PROXY=http://proxy.example.com:8080
# HTTPS_PROXY=http://proxy.example.com:8080
# NO_PROXY=localhost,127.0.0.1,itapprspia01.dci.local
